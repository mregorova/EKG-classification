# -*- coding: utf-8 -*-
"""ecg_classification_simple.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6Oflrs17JOeB4ZndFNxJcXF0I7rCSsS

# Обработка данных
1. Скачиваем данные с *PhysioNet*
2. Делим сигналы так, чтобы каждый содержал одно сердцебиение (window size of 96 points, *sampling ratio* = 128 points/s)
3. Берём только'N' (норма) или 'S' и 'V' (аритмия)
"""

import os
import numpy as np

# install PhysioNet ecg data package
#!pip install wfdb
#import wfdb

# list of available datasets
dbs = wfdb.get_dbs()
display(dbs)

ltafdb_dir = os.path.join(os.getcwd(), 'ltafdb_dir')
wfdb.dl_database('ltafdb', dl_dir=ltafdb_dir)

#mimic_dir = os.path.join(os.getcwd(), 'mimic-iv-ecg-demo_dir')
#wfdb.dl_database('mimic-iv-ecg-demo', dl_dir=mimic_dir)

# Display the downloaded content
ltafdb_in_files = [os.path.splitext(f)[0] for f in os.listdir(ltafdb_dir) if f.endswith('.dat')]
print(ltafdb_in_files)

time_window = 48
all_beats = []
all_annotations = []

for in_file in ltafdb_in_files:
    print('...processing...' + in_file + '...file')
    signal, fields = wfdb.rdsamp(os.path.join(ltafdb_dir,in_file), channels=[0])
    annotations = wfdb.rdann(os.path.join(ltafdb_dir,in_file), 'atr')
    signal=np.array(signal).flatten()

    # grab subsequent heartbeats within [position-48,position+48] window
    #print(len(beats))
    beats = np.zeros((len(annotations.sample[5:-5]), time_window*2))

    # note that we remove first and last few beats to ensure that all beats have equal lengths
    for i, ann_position in enumerate(annotations.sample[5:-5]):
        beats[i] = signal[ann_position-time_window:ann_position+time_window]
    all_beats.append(beats)

    # consequently, we remove first and last few annotations
    all_annotations.append(annotations.symbol[5:-5])

#print(len(all_beats))
all_beats = np.concatenate(all_beats)
all_annotations = np.concatenate(all_annotations)

# check which annotations are usable for us, are of N or S or V class
indices = [i for i, ann in enumerate(all_annotations) if ann in {'N','S','V'}]
# and get only these
all_beats = all_beats[indices]
all_annotations = np.array([all_annotations[i] for i in indices])

# print data statistics
print(all_beats.shape, all_annotations.shape)
print('no of N beats: ' + str(np.count_nonzero(all_annotations == 'N')))
print('no of S beats: ' + str(np.count_nonzero(all_annotations == 'S')))
print('no of V beats: ' + str(np.count_nonzero(all_annotations == 'V')))

# show example samples
!pip install matplotlib==3.1.3
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,3)
fig.set_size_inches(15, 3)
plt.subplots_adjust(wspace=0.2)
print(all_annotations[:100])
sample_number = [0,6,8]
for i, sn in enumerate(sample_number):
    ax[i].plot(all_beats[sn])
    ax[i].set(xlabel='time', ylabel='ecg signal', title='beat type ' + all_annotations[sn])
    ax[i].grid()
plt.show()

"""# Эксперимент

> - Делим данные на train/validation/test, проводим нормировку.
> - Введём метрику точности (датасет не сбалансирован)
>>
```
____Предсказание
И |   n  s  v
с |N  Nn Ns Nv
т |S  Sn Ss Sv
и |V  Vn Vs Vv
н |
а |
```
>> - Общая точность
$Acc_T = \frac{Nn+Ss+Vv}{\Sigma_N+\Sigma_S+\Sigma_V}$,
>> - Точность обнаружения аритмий
$Acc_A = \frac{Ss+Vv}{\Sigma_S+\Sigma_V}$,
>> - $\Sigma_N=Nn+Ns+Nv$, $\Sigma_S=Sn+Ss+Sv$,
$\Sigma_V=Vn+Vs+Vv$

1. Наивный Байес и SVM
2. Решающие деревья с оптимизированным значением max_depth
3. Случайный лес
"""

# prepare datasets and define error metrics
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

# to simplify experiments and speedup training
# we take only some part of the whole dataset
X, y = all_beats[::10], all_annotations[::10]

# train/validation/test set splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15/0.85, random_state=0)
print(len(y_train), len(y_val), len(y_test))

# perform data normalization: z = (x - u)/s
scaler = preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
# same for the validation subset
X_val = preprocessing.StandardScaler().fit_transform(X_val)
# and for the test subset
X_test = preprocessing.StandardScaler().fit_transform(X_test)

# define accuracy
def calculate_accuracy(y_pred, y_gt, comment='', printout=True):
    acc_t = np.count_nonzero(y_pred == y_gt)/len(y_gt)
    acc_a = np.count_nonzero(
        np.logical_and(y_pred == y_gt, y_gt != 'N'))/np.count_nonzero(y_gt != 'N')
    if printout is True:
        print('-----------------------------------')
        print(comment)
        print('Total accuracy, Acc_T = {:.4f}'.format(acc_t))
        print('Arrhythmia accuracy, Acc_A = {:.4f}'.format(acc_a))
        print('-----------------------------------')
    else: return acc_t, acc_a

from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)
calculate_accuracy(y_pred, y_test, comment='naive Bayes classifier')

svc = SVC()
y_pred = svc.fit(X_train, y_train).predict(X_test)
calculate_accuracy(y_pred, y_test, comment='SVM classifier')

svc = SVC(class_weight='balanced')
y_pred = svc.fit(X_train, y_train).predict(X_test)
calculate_accuracy(y_pred, y_test, comment='balanced SVM classifier')

"""Пока не очень:
стремимся получить одинаково хорошие результаты на обеих метриках, но пока наивный Байес показал себя хуже всего (возможно, из-за несбалансированности данных)
"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10)
y_pred = dtc.fit(X_train, y_train).predict(X_test)
calculate_accuracy(y_pred, y_test, comment='balanced DT')

# tunning max_dept hyperparameter (DT likes to overfit)
train_acc_t = []
train_acc_a = []
val_acc_t = []
val_acc_a = []
depth_range = range(1,26)
for max_depth in depth_range:
    dtc = DecisionTreeClassifier(criterion='entropy',
                                 class_weight='balanced',
                                 min_samples_leaf=10,
                                 max_depth=max_depth)
    dt_fit = dtc.fit(X_train, y_train)
    y_pred_train = dt_fit.predict(X_train)
    y_pred_val = dt_fit.predict(X_val)
    acc_t_train, acc_a_train = calculate_accuracy(y_pred_train, y_train, printout=False)
    acc_t_val, acc_a_val = calculate_accuracy(y_pred_val, y_val, printout=False)
    train_acc_t.append(acc_t_train)
    train_acc_a.append(acc_a_train)
    val_acc_t.append(acc_t_val)
    val_acc_a.append(acc_a_val)
    print('{0:d} {1:.4f} {2:4.4f}'.format(max_depth, acc_t_val, acc_a_val))

import matplotlib.pyplot as plt
_, ax = plt.subplots()
ax.plot(depth_range, train_acc_t, label='train acc_t')
ax.plot(depth_range, train_acc_a, label='train acc_a')
ax.plot(depth_range, val_acc_t, label='validation acc_t')
ax.plot(depth_range, val_acc_a , label='validation acc_a')
ax.set(xlabel='max_depth', ylabel='accuracy')
ax.xaxis.set_ticks([1, 5, 10, 15, 20, 25])
ax.legend()
plt.show()

# optimum acc_a max_depth
dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10,
                             max_depth=10)
y_pred = dtc.fit(X_train, y_train).predict(X_test)
calculate_accuracy(y_pred, y_test, comment='DT: Acc_A maximized')

# optimum acc_t & acc_a max_depth
dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10,
                             max_depth=14)
y_pred = dtc.fit(X_train, y_train).predict(X_test)
calculate_accuracy(y_pred, y_test, comment='DT: Acc_T + Acc_A maximized')

# feature vector via PCA (dimensionlality reduction) works poorly
from sklearn.decomposition import PCA
pca = PCA(n_components=15)
X_train_ = pca.fit_transform(X_train)
X_test_ = pca.transform(X_test)

dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10,
                             max_depth=10)
y_pred = dtc.fit(X_train_, y_train).predict(X_test_)
calculate_accuracy(y_pred, y_test, comment='DT with PCA')

"""
1. Решающие деревья работают немного хуже SVM и имеют склонность переобучаться. Рассматриваем два гиперпараметра:
> - *max_depth*
> - *min_samples_leaf*
2. Лучший результат *max_depth* для Acc_A (*max_depth*=8), или Acc_T & Acc_A (*max_depth*=13) даёт максимальные значения
3. PCA тоже особо не сработали"""

import pywt

# extract features using different wavelets and simple differences
def extract_features(input_sample):
    out = np.array([])
# sym8
    cA = pywt.downcoef('a', input_sample, 'sym8', level=4, mode='per')
    out = np.append(out,cA)
    cD = pywt.downcoef('d', input_sample, 'sym8', level=4, mode='per')
    out = np.append(out,cD)
# db6/9
    cA = pywt.downcoef('a', input_sample, 'db6', level=4, mode='per')
    out = np.append(out,cA)
    cD = pywt.downcoef('d', input_sample, 'db6', level=4, mode='per')
    out = np.append(out,cD)
    cA = pywt.downcoef('a', input_sample, 'db9', level=4, mode='per')
    out = np.append(out,cA)
    cD = pywt.downcoef('d', input_sample, 'db9', level=4, mode='per')
    out = np.append(out,cD)
# dmey
    cA = pywt.downcoef('a', input_sample, 'dmey', level=4, mode='per')
    out = np.append(out,cA)
    cD = pywt.downcoef('d', input_sample, 'dmey', level=4, mode='per')
    out = np.append(out,cD)

# differences
    differences = np.zeros(16)
    for i, t in enumerate(range(40, 56)):
        differences[i] = input_sample[t+1]-input_sample[t]
    out = np.append(out,differences)
    return out

# collect vector of features for all samples
def data_features(input_data):
    return np.array([extract_features(sample) for sample in input_data])

X_train_ = data_features(X_train)
print(X_train_.shape)
X_test_ = data_features(X_test)
print(X_test_.shape)

dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10,
                             max_depth=15)
y_pred = dtc.fit(X_train_, y_train).predict(X_test_)
calculate_accuracy(y_pred, y_test, comment='DT with wavelets')

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(criterion='entropy',
                             n_estimators=500,
                             max_depth=10,
                             class_weight='balanced')
y_pred = rfc.fit(X_train_, y_train).predict(X_test_)
calculate_accuracy(y_pred, y_test, comment='RF with wavelets')

from sklearn.ensemble import AdaBoostClassifier

abc = AdaBoostClassifier(n_estimators=200)
y_pred = abc.fit(X_train_, y_train).predict(X_test_)
calculate_accuracy(y_pred, y_test, comment='Ada with wavelets')

"""## 1. Попробуем улучшить классификатор

> ### сбалансируем датасет
"""

# print data statistics
print("\n whole")
print(all_beats.shape, all_annotations.shape)
print('no of N beats: ' + str(np.count_nonzero(all_annotations == 'N')))
print('no of S beats: ' + str(np.count_nonzero(all_annotations == 'S')))
print('no of V beats: ' + str(np.count_nonzero(all_annotations == 'V')))
# print data statistics
print("\n initial")
print(X.shape, y.shape)
print('no of N beats: ' + str(np.count_nonzero(y == 'N')))
print('no of S beats: ' + str(np.count_nonzero(y == 'S')))
print('no of V beats: ' + str(np.count_nonzero(y == 'V')))

"""| Classifier                 | Arrhythmia accuracy, Acc_A | Total accuracy, Acc_T |
|----------------------------|----------------------------|-----------------------|
| naive Bayes                | 0.3919                     | 0.8251                |
| SVM                        | 0.5303                     | **0.9383**                |
| balanced SVM               | **0.7954**                     | 0.9224                |
| balanced DT                | 0.7291                     | 0.8512                |
| DT: Acc_A maximized        | 0.6455                     | 0.8813                |
| DT: Acc_T + Acc_A maximized | 0.7579                     | 0.7587                |
| DT with PCA                | 0.4092                     | 0.5439                |
| DT with wavelets           | 0.7435                     | 0.8545                |
| RF with wavelets           | 0.6801                     | 0.9445                |
| Ada with wavelets          | 0.4755                     | 0.8984                |

"""

y_balanced = []
smallest_size = np.count_nonzero(all_annotations == "V")
a = all_beats[all_annotations == 'V']
print(a.shape)
y_balanced.append(['V']*smallest_size)
b = all_beats[all_annotations == 'S'][0:smallest_size,:]
print(b.shape)
y_balanced.append(['S']*smallest_size)
c = all_beats[all_annotations == 'N'][0:smallest_size,:]
print(c.shape)
y_balanced.append(['N']*smallest_size)


import itertools
y_balanced = list(itertools.chain.from_iterable(y_balanced))
y_balanced = np.array(y_balanced)
X_balanced = np.vstack((a,b,c))
print("\n biggest equal")
print(X_balanced.shape, y_balanced.shape)

# train/validation/test set splitting
X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.15, random_state=0)
X_train_balanced, X_val_balanced, y_train_balanced, y_val_balanced = train_test_split(X_train_balanced, y_train_balanced, test_size=0.15/0.85, random_state=0)
print(len(y_train_balanced), len(y_val_balanced), len(y_test_balanced))

# perform data normalization: z = (x - u)/s
scaler = preprocessing.StandardScaler().fit(X_train_balanced)
X_train_balanced = scaler.transform(X_train_balanced)
# same for the validation subset
X_val_balanced = preprocessing.StandardScaler().fit_transform(X_val_balanced)
# and for the test subset
X_test_balanced = preprocessing.StandardScaler().fit_transform(X_test_balanced)

from sklearn.svm import SVC

svc = SVC()
y_pred_balanced = svc.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='SVM classifier - balanced dataset')

svc = SVC(class_weight='balanced')
y_pred_balanced = svc.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='balanced SVM classifier- balanced dataset')

"""Теперь гораздо лучше

> ### Покрутим гиперпараметры
"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10)
y_pred_balanced = dtc.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='balanced DT')

# tunning max_dept hyperparameter (DT likes to overfit)
train_acc_t = []
train_acc_a = []
val_acc_t = []
val_acc_a = []
depth_range = range(1,26)
for max_depth in depth_range:
    dtc = DecisionTreeClassifier(criterion='entropy',
                                 class_weight='balanced',
                                 min_samples_leaf=10,
                                 max_depth=max_depth)
    dt_fit = dtc.fit(X_train_balanced, y_train_balanced)
    y_pred_train = dt_fit.predict(X_train_balanced)
    y_pred_val = dt_fit.predict(X_val_balanced)
    acc_t_train, acc_a_train = calculate_accuracy(y_pred_train, y_train_balanced, printout=False)
    acc_t_val, acc_a_val = calculate_accuracy(y_pred_val, y_val_balanced, printout=False)
    train_acc_t.append(acc_t_train)
    train_acc_a.append(acc_a_train)
    val_acc_t.append(acc_t_val)
    val_acc_a.append(acc_a_val)
    print('{0:d} {1:.4f} {2:4.4f}'.format(max_depth, acc_t_val, acc_a_val))

import matplotlib.pyplot as plt
_, ax = plt.subplots()
ax.plot(depth_range, train_acc_t, label='train acc_t')
ax.plot(depth_range, train_acc_a, label='train acc_a')
ax.plot(depth_range, val_acc_t, label='validation acc_t')
ax.plot(depth_range, val_acc_a , label='validation acc_a')
ax.set(xlabel='max_depth', ylabel='accuracy')
ax.xaxis.set_ticks([1, 5, 10, 15, 20, 25])
ax.legend()
plt.show()

max_value = max(val_acc_t)
max_index = val_acc_t.index(max_value)
max_value, max_index

val_acc_t[13], val_acc_t[14], val_acc_t[15]

max_value = max(val_acc_a)
max_index = val_acc_a.index(max_value)
max_value, max_index

""" вернём 14 и 19"""

# optimum acc_a max_depth
dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=14,
                             max_depth=19)
y_pred_balanced = dtc.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='DT: 19, balanced')

# optimum acc_t & acc_a max_depth
dtc = DecisionTreeClassifier(criterion='entropy',
                             class_weight='balanced',
                             min_samples_leaf=10,
                             max_depth=14)
y_pred_balanced = dtc.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='DT: Acc_T + Acc_A maximized, balanced')

"""> ### Напишем классификатор MLP"""

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(solver='lbfgs', alpha=1e-42,activation='relu',max_iter=500,
                    hidden_layer_sizes=(5, 3), random_state=1)

y_pred_balanced = mlp.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='mlp, balanced')

mlp2 = MLPClassifier(solver='adam', alpha=1e-4,activation='relu',
                    hidden_layer_sizes=(5, 4), random_state=1)

y_pred_balanced = mlp2.fit(X_train_balanced, y_train_balanced).predict(X_test_balanced)
calculate_accuracy(y_pred_balanced, y_test_balanced, comment='mlp, balanced')
